{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/20242/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20242-NLP-LLM/Part%203%20-%20Introduction%20to%20DL%20and%20LLMs/2-huggingface_tutorial.ipynb\" target=\"_parent\"><img \n",
    "   src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cc50a",
   "metadata": {},
   "source": [
    "### Hugging Face Pipelines Tutorial\n",
    "Hugging Face Pipelines provide a simple, high-level interface for using pre-trained models for various NLP, computer vision, and audio tasks. They abstract away much of the complexity involved in preprocessing, model inference, and postprocessing.\n",
    "\n",
    "#### Installation\n",
    "First, install the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e9a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\me\\myenv310\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in c:\\users\\me\\myenv310\\lib\\site-packages (2.1.2+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\me\\myenv310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\me\\myenv310\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\me\\myenv310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\me\\myenv310\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\myenv310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\me\\myenv310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\me\\myenv310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\me\\myenv310\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\me\\myenv310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\me\\myenv310\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from transformers[torch]) (2.1.2+cpu)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\me\\myenv310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\me\\myenv310\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers[torch])\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\me\\myenv310\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-win_amd64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\me\\myenv310\\lib\\site-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\me\\myenv310\\lib\\site-packages (from requests->transformers[torch]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\me\\myenv310\\lib\\site-packages (from requests->transformers[torch]) (2025.1.31)\n",
      "Requirement already satisfied: sympy in c:\\users\\me\\myenv310\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\me\\myenv310\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\me\\myenv310\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\myenv310\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\me\\myenv310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\me\\myenv310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\me\\myenv310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\me\\myenv310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\me\\myenv310\\lib\\site-packages (from sympy->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-win_amd64.whl (92 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-win_amd64.whl (120 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-win_amd64.whl (45 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-win_amd64.whl (25.8 MB)\n",
      "   ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 3.9/25.8 MB 18.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.9/25.8 MB 21.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.9/25.8 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.8/25.8 MB 20.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.5/25.8 MB 20.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.8 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.8/25.8 MB 19.7 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, accelerate, datasets, evaluate\n",
      "\n",
      "   -- -------------------------------------  1/16 [pyarrow]\n",
      "   -- -------------------------------------  1/16 [pyarrow]\n",
      "   -- -------------------------------------  1/16 [pyarrow]\n",
      "   -- -------------------------------------  1/16 [pyarrow]\n",
      "   -- -------------------------------------  1/16 [pyarrow]\n",
      "   -- -------------------------------------  1/16 [pyarrow]\n",
      "   -- -------------------------------------  1/16 [pyarrow]\n",
      "   -- -------------------------------------  1/16 [pyarrow]\n",
      "   ------- --------------------------------  3/16 [multidict]\n",
      "  Attempting uninstall: fsspec\n",
      "   ------- --------------------------------  3/16 [multidict]\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "   ------- --------------------------------  3/16 [multidict]\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "   ------- --------------------------------  3/16 [multidict]\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "   ------- --------------------------------  3/16 [multidict]\n",
      "   ---------- -----------------------------  4/16 [fsspec]\n",
      "   ---------- -----------------------------  4/16 [fsspec]\n",
      "   ------------ ---------------------------  5/16 [frozenlist]\n",
      "   --------------- ------------------------  6/16 [dill]\n",
      "   --------------- ------------------------  6/16 [dill]\n",
      "   ------------------------- -------------- 10/16 [multiprocess]\n",
      "   ------------------------- -------------- 10/16 [multiprocess]\n",
      "   ------------------------------ --------- 12/16 [aiohttp]\n",
      "   ------------------------------ --------- 12/16 [aiohttp]\n",
      "   ------------------------------ --------- 12/16 [aiohttp]\n",
      "   -------------------------------- ------- 13/16 [accelerate]\n",
      "   -------------------------------- ------- 13/16 [accelerate]\n",
      "   -------------------------------- ------- 13/16 [accelerate]\n",
      "   -------------------------------- ------- 13/16 [accelerate]\n",
      "   -------------------------------- ------- 13/16 [accelerate]\n",
      "   ----------------------------------- ---- 14/16 [datasets]\n",
      "   ----------------------------------- ---- 14/16 [datasets]\n",
      "   ----------------------------------- ---- 14/16 [datasets]\n",
      "   ----------------------------------- ---- 14/16 [datasets]\n",
      "   ----------------------------------- ---- 14/16 [datasets]\n",
      "   ------------------------------------- -- 15/16 [evaluate]\n",
      "   ------------------------------------- -- 15/16 [evaluate]\n",
      "   ---------------------------------------- 16/16 [evaluate]\n",
      "\n",
      "Successfully installed accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 frozenlist-1.6.0 fsspec-2025.3.0 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.0+cpu requires torch==2.7.0, but you have torch 2.1.2+cpu which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers torch\n",
    "\n",
    "# For additional features:\n",
    "! pip install transformers[torch] datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da92c39",
   "metadata": {},
   "source": [
    "Basic Pipeline Usage\n",
    "1. Text Classification (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482d9012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\myenv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997085928916931}]\n",
      "Text: I hate this product\n",
      "Sentiment: NEGATIVE, Score: 0.9998\n",
      "\n",
      "Text: This is amazing!\n",
      "Sentiment: POSITIVE, Score: 0.9999\n",
      "\n",
      "Text: It's okay, nothing special\n",
      "Sentiment: NEGATIVE, Score: 0.8076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Analyze single text\n",
    "result = classifier(\"I love using Hugging Face!\")\n",
    "print(result)\n",
    "# Output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "\n",
    "# Analyze multiple texts\n",
    "texts = [\n",
    "    \"I hate this product\",\n",
    "    \"This is amazing!\",\n",
    "    \"It's okay, nothing special\"\n",
    "]\n",
    "results = classifier(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86de26",
   "metadata": {},
   "source": [
    "2. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df8a7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\me\\myenv310\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\me\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: John\n",
      "Label: PER\n",
      "Score: 0.9985\n",
      "Start: 11, End: 15\n",
      "\n",
      "Entity: New York\n",
      "Label: LOC\n",
      "Score: 0.9990\n",
      "Start: 30, End: 38\n",
      "\n",
      "Entity: Google\n",
      "Label: ORG\n",
      "Score: 0.9986\n",
      "Start: 50, End: 56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NER pipeline\n",
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"My name is John and I live in New York. I work at Google.\"\n",
    "entities = ner(text)\n",
    "\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}\")\n",
    "    print(f\"Label: {entity['entity_group']}\")\n",
    "    print(f\"Score: {entity['score']:.4f}\")\n",
    "    print(f\"Start: {entity['start']}, End: {entity['end']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678db1a",
   "metadata": {},
   "source": [
    "3. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e307d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering pipeline\n",
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications using machine learning. \n",
    "They are especially known for their work in natural language processing. The company was founded in 2016 \n",
    "and is headquartered in New York.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Where is Hugging Face headquartered?\",\n",
    "    \"What is Hugging Face known for?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c24a3",
   "metadata": {},
   "source": [
    "4. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text with custom parameters\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a world where robots exist,\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    for i, gen in enumerate(generated):\n",
    "        print(f\"Generation {i+1}: {gen['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ecb590",
   "metadata": {},
   "source": [
    "5. Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "article = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve \n",
    "from experience without being explicitly programmed. It focuses on the development of computer programs \n",
    "that can access data and use it to learn for themselves. The process of learning begins with observations \n",
    "or data, such as examples, direct experience, or instruction, in order to look for patterns in data and \n",
    "make better decisions in the future based on the examples that we provide. The primary aim is to allow \n",
    "the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(article, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Original length:\", len(article.split()))\n",
    "print(\"Summary:\", summary[0]['summary_text'])\n",
    "print(\"Summary length:\", len(summary[0]['summary_text'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c13ead",
   "metadata": {},
   "source": [
    "6. Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "texts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"I would like to order a coffee.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    translated = translator(text)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"French: {translated[0]['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3ef88",
   "metadata": {},
   "source": [
    "## Advanced Pipeline Usage\n",
    "\n",
    "### Specifying Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b572e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a specific model\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "# Test with social media text\n",
    "tweet = \"Just had the best coffee ever! â˜• #perfect\"\n",
    "result = classifier(tweet)\n",
    "print(f\"Tweet: {tweet}\")\n",
    "print(f\"Sentiment: {result[0]['label']}, Score: {result[0]['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192ee2f",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient batch processing\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"It's okay.\",\n",
    "    \"Absolutely amazing!\",\n",
    "    \"Could be better.\"\n",
    "]\n",
    "\n",
    "# Process in batches for efficiency\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "results = classifier(texts, batch_size=2)\n",
    "\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"{text} -> {result['label']} ({result['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8f07a",
   "metadata": {},
   "source": [
    "### Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b322fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Create pipeline with specific device\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    device=device  # 0 for GPU, -1 for CPU\n",
    ")\n",
    "\n",
    "# The pipeline will now run on GPU if available\n",
    "result = classifier(\"This will run faster on GPU!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d1054",
   "metadata": {},
   "source": [
    "## Computer Vision Pipelines\n",
    "\n",
    "### Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Image classification pipeline\n",
    "classifier = pipeline(\"image-classification\")\n",
    "\n",
    "# Load image from URL\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Classify image\n",
    "results = classifier(image)\n",
    "print(\"Top predictions:\")\n",
    "for result in results[:3]:\n",
    "    print(f\"Label: {result['label']}, Score: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0d2a8",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfee885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection pipeline\n",
    "detector = pipeline(\"object-detection\")\n",
    "\n",
    "# Detect objects in image\n",
    "results = detector(image)\n",
    "\n",
    "print(f\"Found {len(results)} objects:\")\n",
    "for result in results:\n",
    "    box = result['box']\n",
    "    print(f\"Label: {result['label']}\")\n",
    "    print(f\"Score: {result['score']:.4f}\")\n",
    "    print(f\"Box: {box}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d0d6b",
   "metadata": {},
   "source": [
    "## Audio Pipelines\n",
    "\n",
    "### Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21883d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You'll need to install additional dependencies\n",
    "# pip install librosa soundfile\n",
    "\n",
    "# ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\")\n",
    "\n",
    "# Transcribe audio file\n",
    "# audio_file = \"path/to/your/audio.wav\"\n",
    "# result = transcriber(audio_file)\n",
    "# print(f\"Transcription: {result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505a211",
   "metadata": {},
   "source": [
    "## Custom Preprocessing and Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom pipeline with preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove extra whitespace and convert to lowercase\n",
    "    return text.strip().lower()\n",
    "\n",
    "def postprocess_result(result):\n",
    "    # Add custom formatting\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    if score > 0.9:\n",
    "        confidence = \"Very confident\"\n",
    "    elif score > 0.7:\n",
    "        confidence = \"Confident\"\n",
    "    else:\n",
    "        confidence = \"Less confident\"\n",
    "    \n",
    "    return {\n",
    "        'prediction': label,\n",
    "        'confidence_level': confidence,\n",
    "        'raw_score': score\n",
    "    }\n",
    "\n",
    "# Usage example\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "text = \"   THIS IS AMAZING!!!   \"\n",
    "preprocessed = preprocess_text(text)\n",
    "result = classifier(preprocessed)[0]\n",
    "final_result = postprocess_result(result)\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Preprocessed: {preprocessed}\")\n",
    "print(f\"Final result: {final_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58822af8",
   "metadata": {},
   "source": [
    "## Pipeline with Return Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw model outputs\n",
    "classifier = pipeline(\"sentiment-analysis\", return_all_scores=True)\n",
    "\n",
    "text = \"I'm not sure how I feel about this.\"\n",
    "results = classifier(text)\n",
    "\n",
    "print(\"All scores:\")\n",
    "for result in results[0]:  # Pipeline returns list of lists\n",
    "    print(f\"{result['label']}: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d4747",
   "metadata": {},
   "source": [
    "## Error Handling and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abfafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    # Create pipeline with error handling\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    # Handle various input types\n",
    "    inputs = [\n",
    "        \"Normal text\",\n",
    "        \"\",  # Empty string\n",
    "        \"A\" * 1000,  # Very long text\n",
    "        None,  # None value\n",
    "    ]\n",
    "    \n",
    "    for inp in inputs:\n",
    "        try:\n",
    "            if inp is None:\n",
    "                print(f\"Input: {inp} -> Skipped (None value)\")\n",
    "                continue\n",
    "                \n",
    "            if len(inp) == 0:\n",
    "                print(f\"Input: '{inp}' -> Skipped (empty string)\")\n",
    "                continue\n",
    "                \n",
    "            result = classifier(inp)\n",
    "            print(f\"Input: {inp[:50]}... -> {result[0]['label']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing '{inp[:50]}...': {str(e)}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error creating pipeline: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50155913",
   "metadata": {},
   "source": [
    "## Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reuse pipelines instead of creating new ones\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# 2. Use batch processing for multiple inputs\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "results = classifier(texts)  # More efficient than individual calls\n",
    "\n",
    "# 3. Adjust max_length for your use case\n",
    "classifier = pipeline(\"sentiment-analysis\", max_length=128, truncation=True)\n",
    "\n",
    "# 4. Use appropriate model size for your needs\n",
    "# smaller models: distilbert-base-uncased-finetuned-sst-2-english\n",
    "# larger models: roberta-large-mnli"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
