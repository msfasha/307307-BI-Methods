{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/20242/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20242-NLP-LLM/Part%203%20-%20Introduction%20to%20DL%20and%20LLMs/1-Introduction%20to%20DL%20and%20LLMs.ipynb\" target=\"_parent\"><img \n",
    "   src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da4bf6",
   "metadata": {},
   "source": [
    "### Context Aware Word Embeddings - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd402358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: requests in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2e2e8",
   "metadata": {},
   "source": [
    "### Display BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d483e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'bank':\n",
      "tensor([ 4.7019e-01, -1.9835e-01, -1.0122e-01, -1.3519e-01,  1.2612e+00,\n",
      "        -9.6139e-03, -4.9014e-02,  1.0147e+00, -4.5361e-02,  1.7432e-01,\n",
      "         1.2800e-01, -3.2356e-01, -1.3227e-01,  3.6582e-02, -7.8302e-01,\n",
      "        -6.2770e-01,  5.2776e-01,  3.5693e-01,  1.3597e+00,  2.3784e-01,\n",
      "        -3.0995e-01,  4.3136e-02,  3.2358e-01,  3.2144e-01,  3.3207e-01,\n",
      "         4.5470e-01,  6.8660e-01,  5.2037e-01, -2.8076e-01, -5.2107e-01,\n",
      "         5.3412e-01,  9.5313e-01,  3.6960e-01,  4.9074e-01,  1.0348e-01,\n",
      "        -1.2543e-01,  1.8115e-01,  3.9604e-02, -1.1310e+00,  2.2161e-02,\n",
      "        -4.4877e-01, -8.1382e-01, -6.2421e-01,  3.5284e-01, -2.4929e-01,\n",
      "        -6.1539e-01,  1.9276e-01,  2.8171e-01, -7.0082e-01, -8.2422e-01,\n",
      "        -3.0416e-01,  1.0278e+00,  4.3732e-01, -5.0054e-01,  1.1097e-01,\n",
      "         4.7545e-01, -1.0476e+00, -4.6538e-01, -5.3300e-01, -2.1977e-01,\n",
      "         7.0954e-01,  3.1443e-01,  5.0420e-01, -7.7659e-01,  2.3119e-01,\n",
      "        -1.6568e-01,  4.8205e-01, -5.2181e-03, -1.1804e-01, -9.9157e-02,\n",
      "        -2.4582e-01,  1.9177e+00, -8.5401e-01, -9.5023e-01, -4.6143e-01,\n",
      "         5.0577e-01, -2.2459e-01,  2.9306e-01, -2.2287e-01, -9.2279e-03,\n",
      "        -5.4351e-01, -7.2676e-01, -1.5370e-01,  3.8075e-01,  3.8973e-01,\n",
      "        -6.8711e-01,  1.1629e-01,  1.9948e-01, -4.7043e-01,  1.2130e+00,\n",
      "        -1.4237e-01,  2.6370e-01, -9.1200e-01, -2.1185e-01, -1.3493e+00,\n",
      "        -2.0093e-02, -8.5553e-02,  7.4500e-02, -2.7861e-01,  5.7108e-01,\n",
      "         6.0045e-01, -1.1580e+00,  2.2457e-01,  5.2492e-02,  7.7377e-02,\n",
      "         2.1038e-01, -1.0968e+00, -1.8806e-01,  1.6193e-04,  3.0690e-01,\n",
      "         1.5269e-01, -1.2186e+00,  4.4192e-01, -8.6209e-01, -3.1611e-01,\n",
      "        -5.5617e-01,  2.4480e-02, -6.4080e-01, -8.4688e-01, -3.2271e-01,\n",
      "         5.6128e-01,  6.9319e-01,  3.6391e-01,  1.4688e+00,  3.0880e-01,\n",
      "         3.2401e-03,  5.1110e-02,  9.8396e-01, -5.8512e-01, -1.1058e-01,\n",
      "         5.6782e-01,  5.1704e-01,  4.8607e-02,  3.8416e-02,  1.0723e-01,\n",
      "        -5.8880e-01, -3.2970e-03, -4.8399e-01, -1.0776e+00,  3.2018e-01,\n",
      "         8.1521e-01,  8.5122e-01,  5.0063e-01,  1.1447e+00, -1.8171e-01,\n",
      "         4.7930e-01,  6.8499e-02, -9.1607e-01, -2.6628e-01, -1.0907e-01,\n",
      "         1.1996e-01, -3.1764e-01, -3.9093e-01, -5.3363e-01,  2.1021e-01,\n",
      "        -1.9328e-01, -4.6874e-01,  1.2553e-01, -5.8689e-01,  1.6500e-02,\n",
      "         2.6675e-01,  4.8762e-03, -4.1349e-01, -1.0107e-01, -5.9281e-01,\n",
      "        -3.6083e-01,  9.8077e-02,  4.3204e-01, -3.1348e-01,  2.6281e-01,\n",
      "        -4.4020e-01,  9.3242e-02,  1.1335e+00, -3.1242e-01,  9.5865e-01,\n",
      "         4.3642e-01, -9.1704e-01, -5.5271e-01,  5.3453e-01,  8.2077e-01,\n",
      "        -3.4234e-02,  9.8388e-01,  5.4295e-01,  6.9819e-01,  1.0401e+00,\n",
      "         5.3207e-01,  1.7786e-01,  1.1376e-01,  1.5848e-01, -2.3289e-01,\n",
      "        -4.0887e-01, -4.7423e-01, -3.7506e-01,  3.0031e-02, -3.0034e-01,\n",
      "        -3.0897e-01, -3.0131e-01, -2.1677e-01, -8.5815e-01, -3.3625e-01,\n",
      "         1.7334e-01, -1.0664e+00,  2.9628e-01,  1.5440e-01, -8.2941e-02,\n",
      "         1.9345e-01,  4.2606e-01,  3.3704e-01,  1.4883e+00, -1.4554e-01,\n",
      "        -7.5085e-01,  5.5163e-01, -9.2985e-01,  1.2159e-01, -2.5228e-01,\n",
      "         4.6588e-01, -1.2934e-02, -8.3803e-01,  3.8504e-02,  4.2223e-01,\n",
      "         1.9769e-01, -4.9039e-02,  7.5554e-02,  7.1144e-01,  1.0876e-01,\n",
      "         6.0351e-01,  3.1833e-01, -9.8136e-02,  8.2948e-02, -1.1635e-01,\n",
      "         1.1131e-01,  1.5795e-01,  1.0441e+00, -7.2768e-02, -2.5971e-01,\n",
      "        -4.1198e-01, -1.6680e+00, -7.8462e-01, -4.9429e-01, -2.1480e-02,\n",
      "        -4.3348e-01, -3.0725e-01, -2.0169e-01, -3.1562e-01,  5.5264e-01,\n",
      "         8.7739e-01, -2.5813e-01, -4.6994e-01,  2.2665e-01, -6.8825e-01,\n",
      "        -1.0124e-01, -8.5311e-01, -4.8196e-01, -8.7491e-01,  8.3805e-01,\n",
      "        -2.4941e-01, -4.3674e-02,  1.3940e-02, -7.0811e-01,  1.0864e+00,\n",
      "         9.5805e-01, -1.4053e-01,  1.4219e-01,  5.0391e-01, -5.8840e-01,\n",
      "        -6.1770e-01, -4.2367e-02,  2.9746e-01,  3.9472e-01,  6.5948e-01,\n",
      "         6.4870e-01,  2.8621e-01, -5.4719e-01,  1.5666e+00,  7.9899e-01,\n",
      "        -4.2446e-01,  1.0406e-01,  4.4579e-01,  3.1292e-01,  3.9760e-01,\n",
      "         4.6693e-01,  8.0374e-01, -5.1470e-01, -9.7047e-01,  1.6834e-01,\n",
      "        -3.6884e-01, -4.1780e-01,  2.4287e+00, -5.4882e-01, -8.2594e-01,\n",
      "        -1.1033e-01,  1.7993e-01, -5.7966e-01, -1.0053e+00,  5.4360e-02,\n",
      "         8.3745e-01, -1.8572e-02,  1.1848e+00,  1.1216e+00,  9.6871e-02,\n",
      "         6.3155e-01,  1.8133e-01,  8.8556e-01,  1.8355e-02, -5.7767e-01,\n",
      "        -8.8372e-02, -3.8994e-01, -2.1865e-01, -2.6552e+00,  1.2434e+00,\n",
      "        -5.8577e-02, -6.1316e-01,  8.5756e-01,  4.6876e-01,  1.3117e-01,\n",
      "        -4.7934e-01, -9.2976e-01, -4.2786e-01, -1.2408e+00, -1.0706e+00,\n",
      "         8.2764e-01,  4.4291e-01,  9.3939e-03, -4.5428e-01,  5.5158e-01,\n",
      "         4.7812e-01, -2.3170e-01,  7.4008e-01, -8.4432e-02, -8.8175e-01,\n",
      "        -3.7270e-01,  3.4943e-01,  5.0653e-01,  9.5674e-01, -2.6499e-01,\n",
      "         6.0237e-01,  2.9476e-01,  6.6692e-01, -4.0378e-01, -4.7275e-01,\n",
      "         2.9706e-01, -1.0968e-02, -4.7901e-02, -2.0583e-01, -2.4338e-01,\n",
      "        -4.6847e-02,  5.0317e-01, -5.9015e-01,  7.9091e-02,  5.9922e-01,\n",
      "         3.9032e-01,  7.4259e-01,  1.6661e+00, -3.8645e-01,  6.8543e-02,\n",
      "         4.3226e-01, -4.2901e-01,  7.3505e-01,  4.9009e-01,  2.7700e-01,\n",
      "        -3.1625e-01, -2.6357e-01,  5.4675e-01,  6.0926e-01,  4.2210e-01,\n",
      "         9.3381e-01,  3.0226e-01,  3.2522e-02, -5.8780e-01, -2.1337e-01,\n",
      "         2.1275e-01, -2.7158e-01, -2.2421e-01, -4.9844e-01, -1.3258e+00,\n",
      "        -8.7811e-01,  8.5546e-02,  1.9364e-01, -4.5103e-01,  1.1047e-01,\n",
      "         9.4906e-03, -9.3592e-01, -6.6086e-01, -6.2556e-01,  7.9328e-01,\n",
      "         5.0921e-01,  1.6725e-01,  1.2315e-01,  4.3930e-01, -2.6082e-01,\n",
      "         3.3873e-01, -6.6649e-01, -5.6052e-02,  1.3765e-01, -4.9743e-01,\n",
      "        -4.6564e-01, -1.1718e+00, -7.7288e-01,  4.9258e-01,  1.0411e+00,\n",
      "         3.9725e-01, -1.1614e+00, -3.4327e-01,  2.5483e-01, -8.2274e-01,\n",
      "        -2.2485e-01,  3.2781e-03, -1.7640e-01,  4.3532e-02,  1.5662e+00,\n",
      "         1.7924e-01, -5.5276e-02, -2.1097e-01,  9.5104e-03, -3.9506e-01,\n",
      "         1.0361e-01, -1.2626e+00,  8.9531e-01,  1.0269e+00, -1.1605e+00,\n",
      "         1.1192e+00, -9.7013e-01, -5.5257e-02,  8.7882e-01,  6.6064e-01,\n",
      "        -7.3098e-01,  2.5208e-01, -2.0473e-01, -2.0568e-01, -1.6067e-01,\n",
      "        -2.3325e-01, -9.7818e-01, -4.1126e-01, -1.0732e-01,  5.1858e-01,\n",
      "        -8.4810e-01, -5.9603e-01, -4.0070e-01,  5.6848e-01,  1.7639e-01,\n",
      "        -3.6300e-01,  2.1047e-01,  5.3676e-01,  5.3000e-01,  1.9794e-02,\n",
      "        -4.6304e-01,  4.4807e-01, -3.1348e-01,  7.0238e-01,  7.7698e-01,\n",
      "        -2.5306e-01, -5.6150e-02, -7.1272e-01, -5.2060e-01, -7.4560e-01,\n",
      "         4.0661e-01, -1.6192e-03,  4.5005e-02, -8.9174e-01, -8.6501e-02,\n",
      "         3.1608e-01,  2.6019e-01,  3.5316e-01,  1.8502e-02,  7.2444e-01,\n",
      "        -3.1785e-01, -4.5717e-01, -1.0012e+00,  4.5752e-01, -2.4606e-01,\n",
      "         1.2163e+00,  3.8808e-01, -1.1076e+00,  5.3402e-02, -8.9885e-01,\n",
      "        -7.1840e-01,  1.2101e-01,  6.2478e-01, -7.3701e-01, -2.4266e-01,\n",
      "         7.0464e-01,  5.6331e-01,  2.3871e-01,  1.5987e-01, -4.3172e-01,\n",
      "        -2.0836e-01,  4.0169e-01,  5.3638e-02, -1.6520e-01, -2.2658e-01,\n",
      "        -4.9447e-01, -6.9217e-01,  7.6663e-01,  1.1716e+00,  1.9566e-02,\n",
      "         3.8683e-01, -1.7747e-01, -2.4633e-01, -8.0655e-01,  1.0132e+00,\n",
      "        -2.3439e-01, -3.7830e-01, -7.0372e-01, -1.1921e+00,  2.5819e-01,\n",
      "         8.8446e-02, -5.5196e-01,  5.2032e-01, -2.4670e-01,  2.8241e-01,\n",
      "        -2.1413e-01,  1.3306e-01,  2.3173e-01,  6.3649e-01,  4.1812e-03,\n",
      "        -1.5834e-01, -8.4984e-01,  9.9104e-01, -3.0661e-01, -7.2182e-01,\n",
      "        -1.3686e+00, -2.3150e-01, -1.7500e-01, -4.1266e-01,  7.4226e-01,\n",
      "        -3.1678e-01,  1.9286e-01,  3.3245e-01, -8.8161e-01, -6.4049e-01,\n",
      "        -1.2993e-01,  1.5420e-01, -1.5783e+00, -1.8973e-01, -2.3346e-01,\n",
      "         6.4414e-01, -2.8128e-01,  9.2800e-01, -5.1465e-01, -2.2272e-01,\n",
      "         3.5721e-01,  7.8461e-02, -1.1538e-01,  1.1758e-02, -5.2027e-01,\n",
      "        -1.4293e+00,  2.3791e-01, -1.7786e-01, -9.7017e-01, -3.7604e-02,\n",
      "         2.4901e-01,  2.7699e-01, -2.3343e-01,  6.7893e-01,  3.0562e-01,\n",
      "         1.0287e+00, -5.1488e-03,  7.8724e-01,  9.2038e-01,  1.1320e-01,\n",
      "         2.9870e-01,  3.5950e-01, -8.0323e-01,  5.0228e-01, -4.1497e-01,\n",
      "         1.4397e-02,  1.4573e-01,  5.1800e-01,  3.1016e-01, -8.6760e-01,\n",
      "        -6.5939e-01, -5.2958e-03,  7.3445e-01,  9.4478e-03,  1.0323e-01,\n",
      "         4.7411e-01, -2.3847e-02, -3.6831e-01,  4.7018e-01, -1.1345e+00,\n",
      "        -3.2645e-01, -5.1597e-01,  4.1337e-03, -1.5856e-01, -1.6664e+00,\n",
      "        -1.0176e+00,  8.1060e-02,  8.5260e-02, -9.0039e-01,  3.1661e-01,\n",
      "        -2.5163e-01,  6.1876e-01, -9.1736e-01, -9.8474e-02, -9.7167e-01,\n",
      "         2.6413e-01, -5.0671e-01, -5.5369e-01,  2.3438e-01,  9.5259e-01,\n",
      "        -8.3580e-03, -1.9723e-01, -2.3293e-01,  6.7927e-01,  7.5865e-01,\n",
      "        -9.3720e-02,  2.5393e-03,  2.2441e-01,  7.4625e-02, -5.8992e-01,\n",
      "         4.0699e-01,  3.4996e-01,  3.1125e-01,  2.8247e-01, -3.4963e-01,\n",
      "        -2.7337e-01,  3.0089e-01,  4.0446e-01, -1.1853e+00, -1.2064e+00,\n",
      "         6.5552e-01,  2.1645e-01, -1.6714e+00,  1.0232e-01,  2.1132e-02,\n",
      "        -2.5213e-01, -1.1119e+00,  7.0768e-01, -1.5141e-01,  3.0435e-01,\n",
      "         5.7082e-02,  3.1857e-01, -1.2192e-01,  6.6814e-01, -3.6742e-02,\n",
      "        -4.8797e-01,  1.2045e-01, -1.2279e-01, -2.7191e-01,  4.5842e-01,\n",
      "        -4.1853e-01,  1.4959e-01, -8.9580e-03,  2.4565e-01, -3.4461e-01,\n",
      "        -4.1192e-01,  4.5060e-01, -9.1113e-02,  4.8892e-01, -5.1494e-01,\n",
      "        -5.9131e-01,  1.1115e+00,  3.4217e-02, -4.8535e-01,  1.7315e-01,\n",
      "        -2.4216e-01,  9.9814e-01,  5.4380e-01, -1.5811e-03,  4.8767e-01,\n",
      "        -4.8005e-01, -7.4016e-01,  2.6312e-01,  2.4377e-01,  3.4794e-01,\n",
      "        -1.2807e-01, -6.9131e-01,  1.0789e+00,  3.6272e-01,  4.4604e-01,\n",
      "        -3.3334e-01, -6.0666e-01,  6.0547e-02,  2.5235e-01, -3.3373e-01,\n",
      "        -7.2425e-01,  1.9860e-02,  5.7691e-01, -2.3622e-01, -2.7402e-01,\n",
      "         1.2380e+00,  2.7653e-01, -3.1691e-01, -3.7992e-01, -6.5465e-01,\n",
      "         2.4356e-01,  2.4057e-01, -9.3775e-01,  3.3488e-01, -6.2111e-01,\n",
      "        -5.2515e-01, -5.4167e-01, -2.5588e-01, -8.5824e-01,  1.0152e+00,\n",
      "        -6.3203e-01, -1.2432e-01, -4.7801e-01,  2.7275e-01,  5.6323e-01,\n",
      "        -5.5672e-01,  5.2269e-01,  1.9409e-01, -9.8205e-01,  4.7863e-01,\n",
      "         2.5835e-02,  9.7691e-01,  2.1451e-01,  4.0859e-01, -3.5870e-01,\n",
      "         3.2623e-02,  1.3111e-01,  3.7986e-01, -6.5166e-02,  9.7554e-01,\n",
      "        -1.1027e-01, -1.0769e+00,  3.3600e-01,  5.5230e-01,  6.4527e-01,\n",
      "        -4.4907e-01,  1.1377e+00, -3.9828e-01, -7.5065e-01,  4.5029e-01,\n",
      "         2.8535e-01, -2.2207e-01,  6.3101e-02, -8.6954e-02,  6.0205e-01,\n",
      "        -3.4972e-01, -4.2345e-02, -9.0807e-01, -4.0555e-01,  1.6154e-02,\n",
      "         4.2367e-01,  8.8251e-02,  4.9467e-01, -1.1219e+00, -2.9992e-01,\n",
      "         4.9874e-01,  1.6081e+00, -3.0927e-01,  1.9078e-02,  3.3181e-01,\n",
      "         3.2845e-02, -2.6813e-01, -2.6807e-01,  1.1537e+00,  1.5491e-01,\n",
      "         4.2065e-01,  4.3847e-01,  7.1526e-01, -4.2298e-02, -6.4447e-01,\n",
      "        -4.5419e-01, -4.6788e-01, -5.3283e-01, -9.5000e-01, -2.7426e-01,\n",
      "        -2.5471e-01, -5.3061e-01,  1.1504e-01, -2.8412e-01, -4.6136e-01,\n",
      "        -6.2779e-01, -5.1189e-01, -1.1935e-01])\n",
      "\n",
      "Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sentence\n",
    "sentence = \"He went to the bank to deposit money.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Get outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get hidden states (embeddings)\n",
    "embeddings = outputs.last_hidden_state.squeeze(0)  # shape: (seq_len, hidden_size)\n",
    "\n",
    "# Find index of \"bank\"\n",
    "try:\n",
    "    idx = tokens.index(\"bank\")\n",
    "    bank_embedding = embeddings[idx]\n",
    "    print(f\"Embedding for 'bank':\\n{bank_embedding}\\n\\nShape: {bank_embedding.shape}\")\n",
    "except ValueError:\n",
    "    print(\"'bank' not found in tokenized input:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781eefff",
   "metadata": {},
   "source": [
    "#### Use BERT to Create Context-Aware Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21998295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'apple' (fruit) and 'orange': 0.5839\n",
      "Similarity between 'apple' (company) and 'Microsoft': 0.8549\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pretrained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract contextual embedding for a word (handles subwords)\n",
    "def get_token_embedding(sentence, target_word):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    # Tokenize the target word the same way BERT does\n",
    "    target_tokens = tokenizer.tokenize(target_word)\n",
    "\n",
    "    # Search for the position of the target word (handling subwords)\n",
    "    matches = []\n",
    "    for i in range(len(tokens) - len(target_tokens) + 1):\n",
    "        if tokens[i:i + len(target_tokens)] == target_tokens:\n",
    "            matches = list(range(i, i + len(target_tokens)))\n",
    "            break\n",
    "\n",
    "    if not matches:\n",
    "        raise ValueError(f\"'{target_word}' not found in tokens: {tokens}\")\n",
    "\n",
    "    # Average the embeddings over all subword tokens\n",
    "    return embeddings[matches].mean(dim=0)\n",
    "\n",
    "# Contextual sentences\n",
    "sentence_fruit = \"He ate a fresh apple and enjoyed the fruit.\"\n",
    "sentence_company = \"Apple released a new product in the computer market.\"\n",
    "sentence_orange = \"An orange is a juicy fruit.\"\n",
    "sentence_microsoft = \"Microsoft computer was running the latest software.\"\n",
    "\n",
    "# Get embeddings\n",
    "apple_fruit = get_token_embedding(sentence_fruit, \"apple\")\n",
    "apple_company = get_token_embedding(sentence_company, \"apple\")\n",
    "orange = get_token_embedding(sentence_orange, \"orange\")\n",
    "microsoft = get_token_embedding(sentence_microsoft, \"Microsoft\")\n",
    "\n",
    "# Cosine similarity comparisons\n",
    "sim_fruit = F.cosine_similarity(apple_fruit, orange, dim=0)\n",
    "sim_company = F.cosine_similarity(apple_company, microsoft, dim=0)\n",
    "\n",
    "# Results\n",
    "print(f\"Similarity between 'apple' (fruit) and 'orange': {sim_fruit.item():.4f}\")\n",
    "print(f\"Similarity between 'apple' (company) and 'Microsoft': {sim_company.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c8e4b",
   "metadata": {},
   "source": [
    "### Use BERT to Create Questions Answering Applications - Pipeline Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a0246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\myenv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\me\\myenv310\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\me\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a method of pre-training language representations\n",
      "Confidence: 0.6874\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries \n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering \n",
    "from transformers import pipeline \n",
    "import torch \n",
    "\n",
    "# Using pipeline (High-level API) \n",
    "qa_pipeline = pipeline( \"question-answering\",\n",
    "model=\"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\" ) \n",
    "\n",
    "# Example usage \n",
    "context = \"\"\" BERT is a method of pre-training language representations, \n",
    "meaning that it trains a general-purpose language understanding \n",
    "model on a large text corpus (like Wikipedia), \n",
    "and then uses that model for downstream NLP tasks like question answering. \"\"\" \n",
    "\n",
    "question = \"What is BERT?\" \n",
    "result = qa_pipeline(question=question, context=context) \n",
    "print(f\"Answer: {result['answer']}\") \n",
    "print(f\"Confidence: {result['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fad4f2",
   "metadata": {},
   "source": [
    "### BERT Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae83fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required installations (uncomment if not already installed)\n",
    "# !pip install transformers datasets scikit-learn\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file (replace with your actual path)\n",
    "df = pd.read_csv(\"amazon_reviews.csv\")  # Columns: 'title', 'content', 'label'\n",
    "\n",
    "# Combine title and content for input\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"content\"]\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Split into train and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Load BERT model for binary classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer for training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "# Save the model\n",
    "trainer.save_model(\"fine_tuned_bert_amazon_reviews\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv310)",
   "language": "python",
   "name": "myenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
