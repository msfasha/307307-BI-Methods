{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/20242/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20242-NLP-LLM/lecture%20notes/Part%202%20-%20Introduction%20to%20Large%20Language%20Models/introduction_to_llm_python.ipynb\" target=\"_parent\"><img \n",
    "   src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63640f64",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93aa7e7",
   "metadata": {},
   "source": [
    "### Implement the Perceptron using scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59862a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[0.2 0.2]]\n",
      "Bias: [-0.2]\n",
      "Predictions: [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "\n",
    "# Training data for AND gate\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Initialize and train Perceptron\n",
    "model = Perceptron(max_iter=100, eta0=0.1, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Results\n",
    "print(\"Weights:\", model.coef_)\n",
    "print(\"Bias:\", model.intercept_)\n",
    "print(\"Predictions:\", model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b934bb",
   "metadata": {},
   "source": [
    "Note: In the scikit-learn Perceptron, the step function (also called the activation function) is a hard threshold function, and it's built-in.<br>\n",
    "\n",
    "```Python\n",
    "prediction = 1 if output >= 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d83e5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/images/perceptron.png\" alt=\"Simple Perceptron\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c4989",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66abb8",
   "metadata": {},
   "source": [
    "## The Mulit-Layer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711cc96",
   "metadata": {},
   "source": [
    "### Solving the XOR Problem with a Neural Network\n",
    "\n",
    "This code demonstrates how to build and train a simple neural network from scratch using NumPy to learn the XOR logic gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [0 1 1 0]\n",
      "\n",
      "Weights (input to hidden):\n",
      " [ w11 , w12 ]\n",
      "[ w21 , w22 ]\n",
      " [[ 2.7144501   3.27401218]\n",
      " [-2.73418453 -3.17014048]]\n",
      "\n",
      "Bias hidden:\n",
      " [ 1.21994174 -1.63451199]\n",
      "\n",
      "Weights (hidden to output):\n",
      " [[-4.37775211]\n",
      " [ 4.46553876]]\n",
      "\n",
      "Bias output:\n",
      " [3.61855675]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# XOR input and output\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Define MLP with 1 hidden layer of 2 neurons (minimal config for XOR)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(2,), activation='tanh',\n",
    "                    solver='adam', learning_rate_init=0.01,\n",
    "                    max_iter=10000, random_state=42)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = mlp.predict(X)\n",
    "\n",
    "print(\"Predictions:\\n\", predictions)\n",
    "print(\"\\nWeights (input to hidden):\\n\", \"[ w11 , w12 ]\\n[ w21 , w22 ]\\n\", mlp.coefs_[0])\n",
    "print(\"\\nBias hidden:\\n\", mlp.intercepts_[0])\n",
    "print(\"\\nWeights (hidden to output):\\n\", mlp.coefs_[1])\n",
    "print(\"\\nBias output:\\n\", mlp.intercepts_[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c2f18",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/images/mlp.png\" alt=\"Multi Layer Perceptron\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee06af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908610a",
   "metadata": {},
   "source": [
    "## Building Word Embeddings from Scratch\n",
    "We can build word embeddings from sratch using a corpus of our own and using gensim library to build Word2Vec representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1961c",
   "metadata": {},
   "source": [
    "### Example 1 - Simple Tokenized Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ad966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting numpy<2.0,>=1.18.5\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Collecting scipy<1.14.0,>=1.7.0\n",
      "  Downloading scipy-1.13.1-cp310-cp310-win_amd64.whl (46.2 MB)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf78e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'data':\n",
      " [-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
      "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
      " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
      " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
      "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
      "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
      "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
      " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
      "  0.00302193  0.00358008]\n",
      "Words similar to 'data': [('are', 0.16563551127910614), ('fun', 0.13940520584583282), ('learning', 0.1267007291316986), ('powerful', 0.08872982114553452), ('is', 0.011071977205574512), ('and', -0.027849990874528885), ('science', -0.0372748002409935), ('related', -0.15515568852424622), ('machine', -0.2187294214963913)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    ['data', 'science', 'is', 'fun'],\n",
    "    ['machine', 'learning', 'is', 'powerful'],\n",
    "    ['data', 'and', 'learning', 'are', 'related']\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, workers=2)\n",
    "\n",
    "# Access the embedding for a word\n",
    "print(\"Vector for 'data':\\n\", model.wv['data'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"Words similar to 'data':\", model.wv.most_similar('data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f37c1f",
   "metadata": {},
   "source": [
    "### Example 2 - Simple Untokenized Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e48c1",
   "metadata": {},
   "source": [
    "The code below uses the gensim library to buidl word embeddings using Word2Vec models from scratch.<br>\n",
    "It uses a text corpus to learn word similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:/Users/me/AppData/Roaming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd6d098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'language':\n",
      "natural: 0.2196\n",
      "between: 0.2167\n",
      "resources: 0.1955\n",
      "distributed: 0.1696\n",
      "significant: 0.1522\n",
      "\n",
      "Vector for 'business' (first 10 dimensions):\n",
      "[ 0.00816812 -0.00444303  0.00898543  0.00825366 -0.00443522  0.00030311\n",
      "  0.00427449 -0.00392632 -0.00555997 -0.00651232]\n",
      "\n",
      "Analogy results:\n",
      "neural: 0.2595\n",
      "natural: 0.2004\n",
      "resources: 0.1899\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    \"Large language models are transforming business applications\",\n",
    "    \"Natural language processing helps computers understand human language\",\n",
    "    \"Word embeddings capture semantic relationships between words\",\n",
    "    \"Neural networks learn distributed representations of words\",\n",
    "    \"Businesses use language models for various applications\",\n",
    "    \"Customer service can be improved with language technology\",\n",
    "    \"Modern language models require significant computing resources\",\n",
    "    \"Language models can generate human-like text for businesses\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,    # Embedding dimension\n",
    "    window=5,           # Context window size\n",
    "    min_count=1,        # Minimum word frequency\n",
    "    workers=4           # Number of threads\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Find the most similar words to \"language\"\n",
    "similar_words = model.wv.most_similar(\"language\", topn=5)\n",
    "print(\"Words most similar to 'language':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n",
    "\n",
    "# Vector for a specific word\n",
    "word_vector = model.wv[\"business\"]\n",
    "print(f\"\\nVector for 'business' (first 10 dimensions):\\n{word_vector[:10]}\")\n",
    "\n",
    "# Word analogies\n",
    "analogy_result = model.wv.most_similar(\n",
    "    positive=[\"business\", \"language\"],\n",
    "    negative=[\"models\"],\n",
    "    topn=3\n",
    ")\n",
    "print(\"\\nAnalogy results:\")\n",
    "for word, similarity in analogy_result:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6459734",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dafd6d",
   "metadata": {},
   "source": [
    "### Word Similarities Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501a0fa",
   "metadata": {},
   "source": [
    "#### Use Fake Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "033a0a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity(king, queen): 0.9995995265529728\n",
      "Similarity(man, woman): 0.999399810286\n",
      "Similarity(king, apple): 0.5514092058274782\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Fake word vectors (3D for simplicity)\n",
    "word_vectors = {\n",
    "  \"king\": np.array([0.8, 0.65, 0.1]),\n",
    "   \"queen\": np.array([0.78, 0.66, 0.12]), \n",
    "   \"man\": np.array([0.9, 0.1, 0.1]),\n",
    "   \"woman\": np.array([0.88, 0.12, 0.12]),\n",
    "  \"apple\": np.array([0.1, 0.8, 0.9]),\n",
    "}\n",
    "def similarity(w1, w2):\n",
    "\treturn cosine_similarity([word_vectors[w1]], [word_vectors[w2]])[0][0]\n",
    "\n",
    "print(\"Similarity(king, queen):\", similarity(\"king\", \"queen\"))\n",
    "print(\"Similarity(man, woman):\", similarity(\"man\", \"woman\"))\n",
    "print(\"Similarity(king, apple):\", similarity(\"king\", \"apple\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470336fa",
   "metadata": {},
   "source": [
    "#### Use Real Embeddings - Gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2dcc38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'computer': [('computers', 0.7979382276535034), ('laptop', 0.6640493869781494), ('laptop_computer', 0.6548869013786316), ('Computer', 0.647333562374115), ('com_puter', 0.6082078814506531)]\n",
      "king - man + woman = [('queen', 0.7118191123008728)]\n",
      "Vector for 'cat': [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193 ]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Find similar words\n",
    "similar_words = word2vec_model.most_similar('computer', topn=5)\n",
    "print(\"Words similar to 'computer':\", similar_words)\n",
    "\n",
    "# Word analogies\n",
    "result = word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(\"king - man + woman =\", result)\n",
    "\n",
    "# Train your own Word2Vec model\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get vector for a word\n",
    "cat_vector = model.wv['cat']\n",
    "print(\"Vector for 'cat':\", cat_vector[:5])  # Show first 5 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6792dc4",
   "metadata": {},
   "source": [
    "#### Use Real Embeddings - Spacy library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97783fab",
   "metadata": {},
   "source": [
    "Download spacy and the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f770dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.2.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f0dcd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.38253095611315674\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "word1 = nlp(\"king\")\n",
    "word2 = nlp(\"queen\")\n",
    "print(\"Similarity:\", word1.similarity(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540c2c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da4bf6",
   "metadata": {},
   "source": [
    "### Context Aware Word Embeddings - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd402358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Installing collected packages: pyyaml, huggingface-hub, tokenizers, safetensors, transformers\n",
      "Successfully installed huggingface-hub-0.30.2 pyyaml-6.0.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21998295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'apple' (fruit) and 'orange': 0.5839\n",
      "Similarity between 'apple' (company) and 'Microsoft': 0.8549\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pretrained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract contextual embedding for a word (handles subwords)\n",
    "def get_token_embedding(sentence, target_word):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    # Tokenize the target word the same way BERT does\n",
    "    target_tokens = tokenizer.tokenize(target_word)\n",
    "\n",
    "    # Search for the position of the target word (handling subwords)\n",
    "    matches = []\n",
    "    for i in range(len(tokens) - len(target_tokens) + 1):\n",
    "        if tokens[i:i + len(target_tokens)] == target_tokens:\n",
    "            matches = list(range(i, i + len(target_tokens)))\n",
    "            break\n",
    "\n",
    "    if not matches:\n",
    "        raise ValueError(f\"'{target_word}' not found in tokens: {tokens}\")\n",
    "\n",
    "    # Average the embeddings over all subword tokens\n",
    "    return embeddings[matches].mean(dim=0)\n",
    "\n",
    "# Contextual sentences\n",
    "sentence_fruit = \"He ate a fresh apple and enjoyed the fruit.\"\n",
    "sentence_company = \"Apple released a new product in the computer market.\"\n",
    "sentence_orange = \"An orange is a juicy fruit.\"\n",
    "sentence_microsoft = \"Microsoft computer was running the latest software.\"\n",
    "\n",
    "# Get embeddings\n",
    "apple_fruit = get_token_embedding(sentence_fruit, \"apple\")\n",
    "apple_company = get_token_embedding(sentence_company, \"apple\")\n",
    "orange = get_token_embedding(sentence_orange, \"orange\")\n",
    "microsoft = get_token_embedding(sentence_microsoft, \"Microsoft\")\n",
    "\n",
    "# Cosine similarity comparisons\n",
    "sim_fruit = F.cosine_similarity(apple_fruit, orange, dim=0)\n",
    "sim_company = F.cosine_similarity(apple_company, microsoft, dim=0)\n",
    "\n",
    "# Results\n",
    "print(f\"Similarity between 'apple' (fruit) and 'orange': {sim_fruit.item():.4f}\")\n",
    "print(f\"Similarity between 'apple' (company) and 'Microsoft': {sim_company.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
