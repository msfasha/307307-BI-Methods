{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/20242/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20242-NLP-LLM/lecture%20notes/Part%202%20-%20Introduction%20to%20Large%20Language%20Models/introduction_to_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63640f64",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93aa7e7",
   "metadata": {},
   "source": [
    "### Define the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59862a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(self.n_iterations):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = 1 if linear_output >= 0 else 0\n",
    "\n",
    "                # Perceptron update rule\n",
    "                update = self.learning_rate * (y[idx] - y_predicted)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return np.where(linear_output >= 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4075b",
   "metadata": {},
   "source": [
    "### Run The Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a7dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.2 0.1]\n",
      "Bias: -0.20000000000000004\n",
      "Predictions: [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Training data for AND gate\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0], \n",
    "              [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Initialize and train the perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iterations=100)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Display results\n",
    "print(\"Weights:\", perceptron.weights)\n",
    "print(\"Bias:\", perceptron.bias)\n",
    "print(\"Predictions:\", perceptron.predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2116f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66abb8",
   "metadata": {},
   "source": [
    "## The Mulit-Layer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711cc96",
   "metadata": {},
   "source": [
    "### Solving the XOR Problem with a Neural Network\n",
    "\n",
    "This code demonstrates how to build and train a simple neural network from scratch using NumPy to learn the XOR logic gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9423669",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/images/mlp.png\" alt=\"Multi Layer Perceptron\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c7005",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ddd26",
   "metadata": {},
   "source": [
    "### 2. Define the Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f645311",
   "metadata": {},
   "source": [
    "### 3. Define the XOR Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04fe41",
   "metadata": {},
   "source": [
    "### 4. Initialize Network Parameters (The Weights and Biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47dd84d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/images/mlp.png\" alt=\"Multi Layer Perceptron\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# 2 input features, 2 hidden neurons\n",
    "weights_hidden = np.random.uniform(size=(2, 2))\n",
    "\n",
    "# 1 bias for each hidden neuron\n",
    "bias_hidden = np.random.uniform(size=(1, 2)) \n",
    "\n",
    "\n",
    "# 2 hidden neurons, 1 output neuron\n",
    "weights_output = np.random.uniform(size=(2, 1))\n",
    "\n",
    "# 1 bias for output neuron\n",
    "bias_output = np.random.uniform(size=(1, 1))\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc564c6",
   "metadata": {},
   "source": [
    "### 5. Train the Network Using Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Error: 0.4977550305860017\n",
      "Epoch 1001/10000, Error: 0.48962844155619734\n",
      "Epoch 2001/10000, Error: 0.43050559183023696\n",
      "Epoch 3001/10000, Error: 0.3357263739761261\n",
      "Epoch 4001/10000, Error: 0.17357496319517718\n",
      "Epoch 5001/10000, Error: 0.11181272498560178\n",
      "Epoch 6001/10000, Error: 0.08576413241547491\n",
      "Epoch 7001/10000, Error: 0.07130866479694546\n",
      "Epoch 8001/10000, Error: 0.06197519138577699\n",
      "Epoch 9001/10000, Error: 0.055372184098791376\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # input to hidden layer\n",
    "    hidden_layer_input = np.dot(X, weights_hidden) + bias_hidden \n",
    "    \n",
    "    # Activation of hidden layer\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # input to output layer\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_output) + bias_output\n",
    "    \n",
    "    # final output\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Backpropagation\n",
    "    # calculate error, Mean Squared Error (MSE) loss function\n",
    "    error = y - predicted_output\n",
    "    # print error every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Error: {np.mean(np.abs(error))}\")\n",
    "    \n",
    "    # derivative of sigmoid for output layer\n",
    "    d_predicted_output = error * sigmoid_derivative(output_layer_input)\n",
    "    \n",
    "    \n",
    "    # propagate error to hidden layer\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_output.T)\n",
    "    # derivative of sigmoid for hidden layer\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_input)\n",
    "\n",
    "    weights_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate # update weights\n",
    "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate # update bias\n",
    "    weights_hidden += X.T.dot(d_hidden_layer) * learning_rate # update weights\n",
    "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate # update bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cc583",
   "metadata": {},
   "source": [
    "### 6. Evaluate the Final Output\n",
    "#### Run a forward pass to get the final output after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicted output:\n",
      "[[0.053]\n",
      " [0.952]\n",
      " [0.952]\n",
      " [0.052]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final predicted output:\")\n",
    "\n",
    "# input to hidden layer\n",
    "hidden_layer_output = sigmoid(np.dot(X, weights_hidden) + bias_hidden)\n",
    "\n",
    "# final output\n",
    "predicted_output = sigmoid(np.dot(hidden_layer_output, weights_output) + bias_output)\n",
    "print(np.round(predicted_output, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aef165",
   "metadata": {},
   "source": [
    "### 7. Display the Learned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned weights and biases:\n",
      "\n",
      "Hidden layer weights:\n",
      " [[3.79198478 5.81661184]\n",
      " [3.80004873 5.8545897 ]]\n",
      "\n",
      "Hidden layer bias:\n",
      " [[-5.82020057 -2.46277158]]\n",
      "\n",
      "Output layer weights:\n",
      " [[-8.32186051]\n",
      " [ 7.66063503]]\n",
      "\n",
      "Output layer bias:\n",
      " [[-3.45550373]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLearned weights and biases:\")\n",
    "print(\"\\nHidden layer weights:\\n\", weights_hidden)\n",
    "print(\"\\nHidden layer bias:\\n\", bias_hidden)\n",
    "print(\"\\nOutput layer weights:\\n\", weights_output)\n",
    "print(\"\\nOutput layer bias:\\n\", bias_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b5842",
   "metadata": {},
   "source": [
    "### Implementing MCP using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5454db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1]\n",
      "Probabilities:\n",
      " [[0.506 0.494]\n",
      " [0.515 0.485]\n",
      " [0.489 0.511]\n",
      " [0.498 0.502]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define a neural network with 1 hidden layer of 2 neurons\n",
    "model = MLPClassifier(hidden_layer_sizes=(2,),\n",
    "                      activation='logistic',   # sigmoid\n",
    "                      solver='sgd',            # stochastic gradient descent\n",
    "                      learning_rate_init=0.5,\n",
    "                      max_iter=10000,\n",
    "                      random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X)\n",
    "probs = model.predict_proba(X)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Probabilities:\\n\", np.round(probs, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e238bb",
   "metadata": {},
   "source": [
    "Display the learned weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c785e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (coefs_):\n",
      " Layer 0 weights:\n",
      "[[0.005 0.585]\n",
      " [0.283 0.136]]\n",
      " Layer 1 weights:\n",
      "[[-0.717]\n",
      " [ 0.48 ]]\n",
      "\n",
      "Biases (intercepts_):\n",
      " Layer 0 biases:\n",
      "[-0.451 -0.462]\n",
      " Layer 1 biases:\n",
      "[0.069]\n"
     ]
    }
   ],
   "source": [
    "# Display weights and biases\n",
    "print(\"Weights (coefs_):\")\n",
    "for i, coef in enumerate(model.coefs_):\n",
    "    print(f\" Layer {i} weights:\\n{np.round(coef, 3)}\")\n",
    "\n",
    "print(\"\\nBiases (intercepts_):\")\n",
    "for i, intercept in enumerate(model.intercepts_):\n",
    "    print(f\" Layer {i} biases:\\n{np.round(intercept, 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097e003",
   "metadata": {},
   "source": [
    "### Classifying Iris Dataset using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cfb2abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Keep only Setosa (0) and Versicolor (1)\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Feature scaling (important for MLP)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define and train MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,),  # 1 hidden layer, 5 neurons\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    max_iter=1000,\n",
    "                    random_state=42)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
