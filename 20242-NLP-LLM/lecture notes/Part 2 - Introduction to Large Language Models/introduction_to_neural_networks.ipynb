{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/20242/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20242-NLP-LLM/lecture%20notes/Part%202%20-%20Introduction%20to%20Large%20Language%20Models/introduction_to_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63640f64",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93aa7e7",
   "metadata": {},
   "source": [
    "### Define the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59862a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(self.n_iterations):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = 1 if linear_output >= 0 else 0\n",
    "\n",
    "                # Perceptron update rule\n",
    "                update = self.learning_rate * (y[idx] - y_predicted)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return np.where(linear_output >= 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4075b",
   "metadata": {},
   "source": [
    "### Run The Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4a7dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.2 0.1]\n",
      "Bias: -0.20000000000000004\n",
      "Predictions: [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Training data for AND gate\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0], \n",
    "              [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Initialize and train the perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iterations=100)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Display results\n",
    "print(\"Weights:\", perceptron.weights)\n",
    "print(\"Bias:\", perceptron.bias)\n",
    "print(\"Predictions:\", perceptron.predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2116f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66abb8",
   "metadata": {},
   "source": [
    "## The Mulit-Layer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711cc96",
   "metadata": {},
   "source": [
    "### Solving the XOR Problem with a Neural Network\n",
    "\n",
    "This code demonstrates how to build and train a simple neural network from scratch using NumPy to learn the XOR logic gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9423669",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/images/mlp.png\" alt=\"Multi Layer Perceptron\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c7005",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ddd26",
   "metadata": {},
   "source": [
    "### 2. Define the Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f645311",
   "metadata": {},
   "source": [
    "### 3. Define the XOR Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04fe41",
   "metadata": {},
   "source": [
    "### 4. Initialize Network Parameters (The Weights and Biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# 2 input features, 2 hidden neurons\n",
    "weights_hidden = np.random.uniform(size=(2, 2))\n",
    "\n",
    "# 1 bias for each hidden neuron\n",
    "bias_hidden = np.random.uniform(size=(1, 2)) \n",
    "\n",
    "\n",
    "# 2 hidden neurons, 1 output neuron\n",
    "weights_output = np.random.uniform(size=(2, 1))\n",
    "\n",
    "# 1 bias for output neuron\n",
    "bias_output = np.random.uniform(size=(1, 1))\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc564c6",
   "metadata": {},
   "source": [
    "### 5. Train the Network Using Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Error: 0.4977550305860017\n",
      "Epoch 1001/10000, Error: 0.48962844155619734\n",
      "Epoch 2001/10000, Error: 0.43050559183023696\n",
      "Epoch 3001/10000, Error: 0.3357263739761261\n",
      "Epoch 4001/10000, Error: 0.17357496319517718\n",
      "Epoch 5001/10000, Error: 0.11181272498560178\n",
      "Epoch 6001/10000, Error: 0.08576413241547491\n",
      "Epoch 7001/10000, Error: 0.07130866479694546\n",
      "Epoch 8001/10000, Error: 0.06197519138577699\n",
      "Epoch 9001/10000, Error: 0.055372184098791376\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # input to hidden layer\n",
    "    hidden_layer_input = np.dot(X, weights_hidden) + bias_hidden \n",
    "    \n",
    "    # Activation of hidden layer\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # input to output layer\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_output) + bias_output\n",
    "    \n",
    "    # final output\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Backpropagation\n",
    "    # calculate error, Mean Squared Error (MSE) loss function\n",
    "    error = y - predicted_output\n",
    "    # print error every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Error: {np.mean(np.abs(error))}\")\n",
    "    \n",
    "    # derivative of sigmoid for output layer\n",
    "    d_predicted_output = error * sigmoid_derivative(output_layer_input)\n",
    "    \n",
    "    \n",
    "    # propagate error to hidden layer\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_output.T)\n",
    "    # derivative of sigmoid for hidden layer\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_input)\n",
    "\n",
    "    weights_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate # update weights\n",
    "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate # update bias\n",
    "    weights_hidden += X.T.dot(d_hidden_layer) * learning_rate # update weights\n",
    "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate # update bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cc583",
   "metadata": {},
   "source": [
    "### 6. Evaluate the Final Output\n",
    "#### Run a forward pass to get the final output after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicted output:\n",
      "[[0.053]\n",
      " [0.952]\n",
      " [0.952]\n",
      " [0.052]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final predicted output:\")\n",
    "\n",
    "# input to hidden layer\n",
    "hidden_layer_output = sigmoid(np.dot(X, weights_hidden) + bias_hidden)\n",
    "\n",
    "# final output\n",
    "predicted_output = sigmoid(np.dot(hidden_layer_output, weights_output) + bias_output)\n",
    "print(np.round(predicted_output, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aef165",
   "metadata": {},
   "source": [
    "### 7. Display the Learned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned weights and biases:\n",
      "\n",
      "Hidden layer weights:\n",
      " [[3.79198478 5.81661184]\n",
      " [3.80004873 5.8545897 ]]\n",
      "\n",
      "Hidden layer bias:\n",
      " [[-5.82020057 -2.46277158]]\n",
      "\n",
      "Output layer weights:\n",
      " [[-8.32186051]\n",
      " [ 7.66063503]]\n",
      "\n",
      "Output layer bias:\n",
      " [[-3.45550373]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLearned weights and biases:\")\n",
    "print(\"\\nHidden layer weights:\\n\", weights_hidden)\n",
    "print(\"\\nHidden layer bias:\\n\", bias_hidden)\n",
    "print(\"\\nOutput layer weights:\\n\", weights_output)\n",
    "print(\"\\nOutput layer bias:\\n\", bias_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
