{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ff2b15",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Classifier with Bag of Words and Word Embeddings\n",
    "\n",
    "### 1. Open Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88c9fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dff1343a-7c6b-43d9-9d03-56a7a2c6182d",
       "rows": [
        [
         "0",
         "1",
         "Toasts great but difficult to remove English muffins",
         "I love the way this toaster evenly toasts bread, bagels and english muffins but wish it would raise higher in order to remove the english muffins."
        ],
        [
         "1",
         "1",
         "round the outside.. round the outside...",
         "This is a very important record, for a number of reasons. It introduced the world to hip-hop. It did 'world music' before it even had a name. It beat Paul Simon to 'discovering' South African music by years... Keith Haring did the cover art. I bought it when it came out for 'Buffalo Girls' after seeing Malcolm Mclaren talking about 'scratching' on the Tube (CH4 TV series in the UK). It's still one of my favorite albums of all time. Most of the credit should go to Trevor Horn - Mclaren ran out of ideas and Horn saved this. It has to be one of the most influential albums of it's time."
        ],
        [
         "2",
         "0",
         "What is this book's appeal?",
         "I feel a little silly to be about the only one who doesn't get this book's appeal. It shows the same pictures over and over again, and every other page is black and white. No appeal whatsoever to my daughter, now age 18 months, who has had this book since birth. People say it's the 'lulling\" repeatability of the goodnight theme... I say, it's boring. There are so many bright, lively, board books out there today. I do not recommend this one, classic or not."
        ],
        [
         "3",
         "1",
         "A Must Have for Literacy Instruction",
         "Excellent resource for those teachers wanting to grow in their literacy instruction. If you feel like you have to reenergize your reading and writing program then this is a great book for you. I wish I had this book when I was a first year and would reccomend it for any teachers just starting out."
        ],
        [
         "4",
         "1",
         "SADE: Lovers Rock",
         "What can I say...this woman is absolutely incredible! If you ever get a chance to see her live, YOU MUST GO! She is worth every penny!"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toasts great but difficult to remove English m...</td>\n",
       "      <td>I love the way this toaster evenly toasts brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>round the outside.. round the outside...</td>\n",
       "      <td>This is a very important record, for a number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>What is this book's appeal?</td>\n",
       "      <td>I feel a little silly to be about the only one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A Must Have for Literacy Instruction</td>\n",
       "      <td>Excellent resource for those teachers wanting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>SADE: Lovers Rock</td>\n",
       "      <td>What can I say...this woman is absolutely incr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      1  Toasts great but difficult to remove English m...   \n",
       "1      1           round the outside.. round the outside...   \n",
       "2      0                        What is this book's appeal?   \n",
       "3      1               A Must Have for Literacy Instruction   \n",
       "4      1                                  SADE: Lovers Rock   \n",
       "\n",
       "                                             content  \n",
       "0  I love the way this toaster evenly toasts brea...  \n",
       "1  This is a very important record, for a number ...  \n",
       "2  I feel a little silly to be about the only one...  \n",
       "3  Excellent resource for those teachers wanting ...  \n",
       "4  What can I say...this woman is absolutely incr...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = \"\"\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\me\\\\Downloads\\\\chunk_0000.csv\")\n",
    "\n",
    "# Display a few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15f17a",
   "metadata": {},
   "source": [
    "### 2. Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e51ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge title and content into one text field\n",
    "df['text'] = df['title'].fillna('') + \" \" + df['content'].fillna('')\n",
    "\n",
    "# Features and labels\n",
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb076103",
   "metadata": {},
   "source": [
    "### Part A: Sentiment Classifier using Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5db086",
   "metadata": {},
   "source": [
    "#### 3A. BoW Feature Extraction and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba19101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Classifier Results\n",
      "Accuracy: 0.88745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      9947\n",
      "           1       0.89      0.89      0.89     10053\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.89      0.89      0.89     20000\n",
      "weighted avg       0.89      0.89      0.89     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Bag of Words vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform text into BoW vectors\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "bow_model = LogisticRegression(max_iter=1000)\n",
    "bow_model.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_bow = bow_model.predict(X_test_bow)\n",
    "\n",
    "print(\"BoW Classifier Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9951dd",
   "metadata": {},
   "source": [
    "### Part B: Sentiment Classifier using Word Embeddings (Gensim)\n",
    "#### 3B. Word Embedding Feature Extraction and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb929c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.downloader import load\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6b449",
   "metadata": {},
   "source": [
    "### Load pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6f58b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "Embedding Classifier Results\n",
      "Accuracy: 0.801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80      9947\n",
      "           1       0.80      0.80      0.80     10053\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.80      0.80      0.80     20000\n",
      "weighted avg       0.80      0.80      0.80     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.downloader import load\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word2vec_model = load('glove-wiki-gigaword-100')  # 100-dimensional GloVe vectors\n",
    "\n",
    "# Helper function: average word embeddings for a text\n",
    "def get_avg_word2vec(text, model, k=100):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            vectors.append(model[token])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(k)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# Create embeddings for all texts\n",
    "X_vectors = np.vstack([get_avg_word2vec(text, word2vec_model) for text in X])\n",
    "\n",
    "# Train/test split for embeddings\n",
    "X_train_vec, X_test_vec, y_train_vec, y_test_vec = train_test_split(X_vectors, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "embedding_model = LogisticRegression(max_iter=1000)\n",
    "embedding_model.fit(X_train_vec, y_train_vec)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_vec = embedding_model.predict(X_test_vec)\n",
    "\n",
    "print(\"Embedding Classifier Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_vec, y_pred_vec))\n",
    "print(classification_report(y_test_vec, y_pred_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c46c2b",
   "metadata": {},
   "source": [
    "### Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5089e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Results:\n",
      "BoW Accuracy: 0.8874\n",
      "Embedding Accuracy: 0.8010\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSummary of Results:\")\n",
    "print(f\"BoW Accuracy: {accuracy_score(y_test, y_pred_bow):.4f}\")\n",
    "print(f\"Embedding Accuracy: {accuracy_score(y_test_vec, y_pred_vec):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1cd13",
   "metadata": {},
   "source": [
    "## Real technical reasons why BoW still beats embeddings, even with 10,000 samples:\n",
    "\n",
    "### 1. **Averaging word embeddings destroys word order and negation**\n",
    "- When you average vectors, \"good\" and \"not good\" look almost the same.\n",
    "- Sentiment relies heavily on small words like \"not\", \"never\", \"but\", etc., which are lost by naive averaging.\n",
    "- Bag of Words keeps \"not good\" explicitly separate as two words — so it captures this better.\n",
    "\n",
    "### 2. **Embeddings are trained for semantic similarity, not for sentiment polarity**\n",
    "- GloVe and Word2Vec embeddings are trained to capture **meaning similarity**, not **sentiment**.\n",
    "- Example: \"good\" and \"bad\" might end up closer than you want because both are adjectives describing quality.\n",
    "- BoW directly catches \"bad\" and \"good\" as separate dimensions.\n",
    "\n",
    "### 3. **Pre-trained embeddings are not customized to Amazon review domain**\n",
    "- GloVe was trained on Wikipedia and news, not on Amazon-style informal, product-centric reviews.\n",
    "- Words like \"warranty\", \"battery\", \"durable\", \"refund\", \"defective\" may not be well-represented.\n",
    "\n",
    "### 4. **Simple Logistic Regression on top of averaged embeddings is too weak**\n",
    "- More powerful models like **CNNs** or **LSTMs** over the sequences of embeddings could do better.\n",
    "- Logistic Regression assumes a linear boundary, and embeddings alone may not separate classes linearly enough.\n",
    "\n",
    "## How to fix this or improve the embedding model:\n",
    "\n",
    "Here are a few good strategies:\n",
    "\n",
    "| Strategy | Why It Helps |\n",
    "|:---------|:-------------|\n",
    "| TF-IDF Weighted Averaging | Gives more importance to rare, meaningful words and less to common words. |\n",
    "| Fine-tuning embeddings | Updates embeddings during model training to capture review-specific language. |\n",
    "| Train a small neural network | Instead of Logistic Regression, use a feedforward network or LSTM/GRU. |\n",
    "| Use Doc2Vec (Paragraph Vector) | Learns a direct fixed-size vector for the whole document without averaging words. |\n",
    "| Train embeddings on your Amazon data | Get domain-specific representations of words. |\n",
    "\n",
    "### Summary:\n",
    "\n",
    "> \"Pre-trained embeddings are not magic. They help when you can use a model that understands sequences and context. For many traditional machine learning models like Logistic Regression, a simple Bag of Words representation can outperform averaged embeddings because it preserves discriminative keywords better.\"\n",
    "\n",
    "### We can try:\n",
    "- TF-IDF weighted word2vec averaging method, or\n",
    "- Full example of training a **Doc2Vec** model using Gensim.\n",
    "\n",
    "These would get you much closer to beating the BoW model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce284f",
   "metadata": {},
   "source": [
    "### Additional Analysis: Word Importance in BoW Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f78005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Positive Words:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "coefficient",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "04e1ca5a-6141-4326-8e0e-f77cd02e03cb",
       "rows": [
        [
         "70111",
         "pleasantly",
         "2.068650527085809"
        ],
        [
         "9270",
         "awesome",
         "1.9230919257397006"
        ],
        [
         "36311",
         "flawless",
         "1.9115602158105534"
        ],
        [
         "89903",
         "susi",
         "1.893055597339128"
        ],
        [
         "35040",
         "fave",
         "1.7749133919416689"
        ],
        [
         "67828",
         "paulina",
         "1.7719083304227567"
        ],
        [
         "42831",
         "haters",
         "1.7575464435497923"
        ],
        [
         "33575",
         "exellent",
         "1.7466874341459893"
        ],
        [
         "48745",
         "invaluable",
         "1.7390127661243848"
        ],
        [
         "16199",
         "captures",
         "1.7257942006496179"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70111</th>\n",
       "      <td>pleasantly</td>\n",
       "      <td>2.068651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1.923092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36311</th>\n",
       "      <td>flawless</td>\n",
       "      <td>1.911560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89903</th>\n",
       "      <td>susi</td>\n",
       "      <td>1.893056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35040</th>\n",
       "      <td>fave</td>\n",
       "      <td>1.774913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67828</th>\n",
       "      <td>paulina</td>\n",
       "      <td>1.771908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42831</th>\n",
       "      <td>haters</td>\n",
       "      <td>1.757546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33575</th>\n",
       "      <td>exellent</td>\n",
       "      <td>1.746687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48745</th>\n",
       "      <td>invaluable</td>\n",
       "      <td>1.739013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16199</th>\n",
       "      <td>captures</td>\n",
       "      <td>1.725794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  coefficient\n",
       "70111  pleasantly     2.068651\n",
       "9270      awesome     1.923092\n",
       "36311    flawless     1.911560\n",
       "89903        susi     1.893056\n",
       "35040        fave     1.774913\n",
       "67828     paulina     1.771908\n",
       "42831      haters     1.757546\n",
       "33575    exellent     1.746687\n",
       "48745  invaluable     1.739013\n",
       "16199    captures     1.725794"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Negative Words:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "coefficient",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "96530977-834b-4131-9be7-8fcbf56c9480",
       "rows": [
        [
         "27424",
         "disappointment",
         "-3.143103726405755"
        ],
        [
         "27421",
         "disappointing",
         "-2.67899290462863"
        ],
        [
         "70748",
         "poorly",
         "-2.5241827095010323"
        ],
        [
         "102630",
         "worst",
         "-2.4311826056601267"
        ],
        [
         "13415",
         "boring",
         "-2.2751515608293675"
        ],
        [
         "27387",
         "disapointing",
         "-2.2136754291651832"
        ],
        [
         "53743",
         "lesley",
         "-2.2103983516988177"
        ],
        [
         "27983",
         "dissapointment",
         "-2.196628489707945"
        ],
        [
         "102651",
         "worthless",
         "-2.180397243948261"
        ],
        [
         "89096",
         "sucks",
         "-2.1772898150885895"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27424</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>-3.143104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27421</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>-2.678993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70748</th>\n",
       "      <td>poorly</td>\n",
       "      <td>-2.524183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102630</th>\n",
       "      <td>worst</td>\n",
       "      <td>-2.431183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13415</th>\n",
       "      <td>boring</td>\n",
       "      <td>-2.275152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27387</th>\n",
       "      <td>disapointing</td>\n",
       "      <td>-2.213675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53743</th>\n",
       "      <td>lesley</td>\n",
       "      <td>-2.210398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27983</th>\n",
       "      <td>dissapointment</td>\n",
       "      <td>-2.196628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102651</th>\n",
       "      <td>worthless</td>\n",
       "      <td>-2.180397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89096</th>\n",
       "      <td>sucks</td>\n",
       "      <td>-2.177290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  coefficient\n",
       "27424   disappointment    -3.143104\n",
       "27421    disappointing    -2.678993\n",
       "70748           poorly    -2.524183\n",
       "102630           worst    -2.431183\n",
       "13415           boring    -2.275152\n",
       "27387     disapointing    -2.213675\n",
       "53743           lesley    -2.210398\n",
       "27983   dissapointment    -2.196628\n",
       "102651       worthless    -2.180397\n",
       "89096            sucks    -2.177290"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get feature importance\n",
    "coefficients = bow_model.coef_[0]\n",
    "words = vectorizer.get_feature_names_out()\n",
    "word_importance = pd.DataFrame({'word': words, 'coefficient': coefficients})\n",
    "\n",
    "# Top 10 Positive Words\n",
    "print(\"Top Positive Words:\")\n",
    "display(word_importance.sort_values(by='coefficient', ascending=False).head(10))\n",
    "\n",
    "# Top 10 Negative Words\n",
    "print(\"Top Negative Words:\")\n",
    "display(word_importance.sort_values(by='coefficient', ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2475415",
   "metadata": {},
   "source": [
    "### Benefits of Word Embeddings for Downstream Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f60e6",
   "metadata": {},
   "source": [
    "Applications where Word Embeddings out performs BoW\n",
    "\n",
    "| Application | Why Word Embeddings Help |\n",
    "|:------------|:--------------------------|\n",
    "| **Semantic Search / Similarity** | Embeddings allow you to retrieve similar documents, not just keyword matches. |\n",
    "| **Clustering Reviews / Topics** | You can group related reviews even if they use different words (synonyms). |\n",
    "| **Text Recommendation Systems** | Embeddings allow recommending similar products/reviews based on meaning, not keywords. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525e034",
   "metadata": {},
   "source": [
    "Given a random review (\"The battery life is excellent\"),  \n",
    "use word embeddings to **retrieve the top 5 most semantically similar reviews** — even if they don't use the word \"battery\" exactly.\n",
    "\n",
    "**With BoW:** only exact words match.\n",
    "\n",
    "**With Embeddings:** similar meaning matches (e.g., \"long lasting\", \"holds charge\", \"durable\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assume you already have:\n",
    "# - X_vectors: embedding vectors for each review\n",
    "# - df['text']: the original texts\n",
    "\n",
    "# Choose a random review\n",
    "idx = 100  # Pick any index\n",
    "query_vector = X_vectors[idx].reshape(1, -1)\n",
    "query_text = df['text'].iloc[idx]\n",
    "\n",
    "# Compute similarity\n",
    "similarities = cosine_similarity(query_vector, X_vectors)[0]\n",
    "\n",
    "# Get top 5 similar reviews\n",
    "top_indices = similarities.argsort()[-6:-1][::-1]  # Top 5 excluding itself\n",
    "similar_texts = df['text'].iloc[top_indices]\n",
    "\n",
    "print(\"Query Review:\")\n",
    "print(query_text)\n",
    "print(\"\\nMost Similar Reviews (using Word Embeddings):\")\n",
    "for i, text in enumerate(similar_texts):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dfa45b",
   "metadata": {},
   "source": [
    "# Visual Result Example\n",
    "\n",
    "> Query: \"The battery lasts a long time and charges quickly.\"  \n",
    "> Similar Reviews:\n",
    "> 1. \"Holds charge for days. Very impressed.\"\n",
    "> 2. \"Long battery life and fast charging feature.\"\n",
    "> 3. \"Impressive durability and energy efficiency.\"\n",
    "> 4. \"The battery power is very reliable.\"\n",
    "> 5. \"Stays charged all weekend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "**Notice:** Different words, same meaning.  \n",
    "Bag of Words would totally miss this.\n",
    "\n",
    "\n",
    "# Why this is perfect for students:\n",
    "- Shows a **clear weakness** of BoW.\n",
    "- Shows **real-world value** of embeddings.\n",
    "- Shows why embeddings are **essential** for more advanced tasks beyond simple classification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv310)",
   "language": "python",
   "name": "myenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
