{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc8bfb8",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Semantic Similarity with Bag of Words and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f641f",
   "metadata": {},
   "source": [
    "\n",
    "This notebook covers two key parts:\n",
    "\n",
    "1. Sentiment Analysis using Bag of Words and Word Embeddings.\n",
    "2. Semantic Similarity Search using Word Embeddings.\n",
    "\n",
    "We will demonstrate the strengths and weaknesses of both representations in real downstream applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3c084",
   "metadata": {},
   "source": [
    "## 1. File Upload and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from IPython.display import display\n",
    "\n",
    "# Upload CSV file dynamically\n",
    "uploaded = files.upload()\n",
    "\n",
    "# List uploaded files\n",
    "for filename in uploaded.keys():\n",
    "    print(f'User uploaded file \"{filename}\"')\n",
    "\n",
    "# Load the selected file\n",
    "csv_file = list(uploaded.keys())[0]\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Display a few rows\n",
    "display(df.head())\n",
    "\n",
    "# Merge title and content\n",
    "df['text'] = df['title'].fillna('') + \" \" + df['content'].fillna('')\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1ed64",
   "metadata": {},
   "source": [
    "# Part 1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3d620",
   "metadata": {},
   "source": [
    "## 2A. Sentiment Analysis using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f41b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bag of Words vectorization\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "bow_model = LogisticRegression(max_iter=1000)\n",
    "bow_model.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_bow = bow_model.predict(X_test_bow)\n",
    "\n",
    "print(\"BoW Classifier Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(classification_report(y_test, y_pred_bow))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b2d2f",
   "metadata": {},
   "source": [
    "## 2B. Sentiment Analysis using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.downloader import load\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word2vec_model = load('glove-wiki-gigaword-100')\n",
    "\n",
    "# Helper function to average word embeddings\n",
    "def get_avg_word2vec(text, model, k=100):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            vectors.append(model[token])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(k)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# Build vectors\n",
    "X_vectors = np.vstack([get_avg_word2vec(text, word2vec_model) for text in X])\n",
    "\n",
    "# Split again\n",
    "X_train_vec, X_test_vec, y_train_vec, y_test_vec = train_test_split(X_vectors, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression\n",
    "embedding_model = LogisticRegression(max_iter=1000)\n",
    "embedding_model.fit(X_train_vec, y_train_vec)\n",
    "\n",
    "# Predictions\n",
    "y_pred_vec = embedding_model.predict(X_test_vec)\n",
    "\n",
    "print(\"Embedding Classifier Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_vec, y_pred_vec))\n",
    "print(classification_report(y_test_vec, y_pred_vec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22723d30",
   "metadata": {},
   "source": [
    "## 3. Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7570023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(f\"BoW Accuracy: {accuracy_score(y_test, y_pred_bow):.4f}\")\n",
    "print(f\"Embedding Accuracy: {accuracy_score(y_test_vec, y_pred_vec):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75aef0",
   "metadata": {},
   "source": [
    "# Part 2: Semantic Similarity Search with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Choose a random review\n",
    "idx = 100  # or any valid index\n",
    "query_vector = X_vectors[idx].reshape(1, -1)\n",
    "query_text = df['text'].iloc[idx]\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(query_vector, X_vectors)[0]\n",
    "\n",
    "# Get top 5 most similar reviews (excluding the query itself)\n",
    "top_indices = similarities.argsort()[-6:-1][::-1]\n",
    "similar_texts = df['text'].iloc[top_indices]\n",
    "\n",
    "print(\"Query Review:\")\n",
    "print(query_text)\n",
    "print(\"\\nMost Similar Reviews (using Word Embeddings):\")\n",
    "for i, text in enumerate(similar_texts):\n",
    "    print(f\"{i+1}. {text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc71f41",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "Bag of Words models are strong for simple classification tasks where important information is contained in the presence of specific keywords.\n",
    "\n",
    "Word embeddings become essential in downstream tasks that require understanding of semantic similarity, such as:\n",
    "\n",
    "- Semantic search\n",
    "- Text clustering\n",
    "- Recommendation systems\n",
    "- Transfer learning\n",
    "\n",
    "Embeddings allow capturing meaning beyond surface-level word matching.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
