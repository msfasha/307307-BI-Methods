{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/20242/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20242-NLP-LLM/lecture%20notes/Part%202%20-%20Introduction%20to%20Large%20Language%20Models/introduction_to_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63640f64",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93aa7e7",
   "metadata": {},
   "source": [
    "### Implement the Perceptron using scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59862a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[0.2 0.2]]\n",
      "Bias: [-0.2]\n",
      "Predictions: [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "\n",
    "# Training data for AND gate\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Initialize and train Perceptron\n",
    "model = Perceptron(max_iter=100, eta0=0.1, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Results\n",
    "print(\"Weights:\", model.coef_)\n",
    "print(\"Bias:\", model.intercept_)\n",
    "print(\"Predictions:\", model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b934bb",
   "metadata": {},
   "source": [
    "Note: In the scikit-learn Perceptron, the step function (also called the activation function) is a hard threshold function, and it's built-in.<br>\n",
    "\n",
    "```Python\n",
    "prediction = 1 if output >= 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c4989",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66abb8",
   "metadata": {},
   "source": [
    "## The Mulit-Layer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711cc96",
   "metadata": {},
   "source": [
    "### Solving the XOR Problem with a Neural Network\n",
    "\n",
    "This code demonstrates how to build and train a simple neural network from scratch using NumPy to learn the XOR logic gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [0 1 1 0]\n",
      "\n",
      "Weights (input to hidden):\n",
      " [[ 2.7144501   3.27401218]\n",
      " [-2.73418453 -3.17014048]]\n",
      "\n",
      "Bias hidden:\n",
      " [ 1.21994174 -1.63451199]\n",
      "\n",
      "Weights (hidden to output):\n",
      " [[-4.37775211]\n",
      " [ 4.46553876]]\n",
      "\n",
      "Bias output:\n",
      " [3.61855675]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# XOR input and output\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Define MLP with 1 hidden layer of 2 neurons (minimal config for XOR)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(2,), activation='tanh',\n",
    "                    solver='adam', learning_rate_init=0.01,\n",
    "                    max_iter=10000, random_state=42)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = mlp.predict(X)\n",
    "\n",
    "print(\"Predictions:\\n\", predictions)\n",
    "print(\"\\nWeights (input to hidden):\\n\", mlp.coefs_[0])\n",
    "print(\"\\nBias hidden:\\n\", mlp.intercepts_[0])\n",
    "print(\"\\nWeights (hidden to output):\\n\", mlp.coefs_[1])\n",
    "print(\"\\nBias output:\\n\", mlp.intercepts_[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4841f",
   "metadata": {},
   "source": [
    "        H1     H2\n",
    "I1   [ w00 , w01 ]\n",
    "I2   [ w10 , w11 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c2f18",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/images/mlp.png\" alt=\"Multi Layer Perceptron\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d5144",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
